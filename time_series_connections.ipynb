{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IslandTime import TimeSeriesConnections, retrieve_island_info, plot_shoreline_transects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coastline_position_transect_2_waterline',\n",
       "       'sea_surface_temperature_NOAACRW', 'sea_level_anomaly',\n",
       "       '2_metre_dewpoint_temperature', 'soil_temperature_level_1',\n",
       "       'total_precipitation', 'evaporation', 'sea_surface_temperature',\n",
       "       'mean_sea_level_pressure', 'mean_direction_of_total_swell',\n",
       "       'mean_direction_of_wind_waves', 'mean_wave_direction', 'wind_speed_10m',\n",
       "       'wind_direction_10m', 'wind_direction_true_10m',\n",
       "       'wave_energy_of_total_swell', 'wave_energy_of_wind_waves',\n",
       "       'wave_energy_of_combined_wind_waves_and_swell'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "island_info['timeseries_preprocessing']['optimal time period']['dict_timeseries']['coastline_position_transect_2_waterline']['monthly'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "island_info = retrieve_island_info('Dhakandhoo', 'Maldives', verbose=False)\n",
    "\n",
    "ts1 = island_info['timeseries_preprocessing']['optimal time period']['dict_timeseries']['coastline_position_transect_2_waterline']['monthly']['coastline_position_transect_2_waterline']\n",
    "ts2 = island_info['timeseries_preprocessing']['optimal time period']['dict_timeseries']['coastline_position_transect_2_waterline']['monthly']['wave_energy_of_combined_wind_waves_and_swell']\n",
    "\n",
    "ts_dict = {'ts1': ts1, 'ts2': ts2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------\n",
      "Evaluating time series connections\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "[('ts1', 'ts2')]\n",
      "--- Evaluating Granger causality ---\n",
      "--- Evaluating cross-correlation ---\n",
      "--- Evaluating correlation ---\n",
      "Pearson correlation coefficient:  -0.11425104269016166\n",
      "--- Evaluating cointegration ---\n",
      " t statistic: -1.78 \n",
      " p value: 0.64 \n",
      " critical p values [1%, 5%, 10%] [-4.02083922 -3.40409789 -3.09138506]\n"
     ]
    }
   ],
   "source": [
    "dictt = TimeSeriesConnections(ts_dict).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "ts1.plot(ax=ax, color='r')\n",
    "ts2.plot(ax=ax2, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t statistic: -3.83 \n",
      " p value: 0.04 \n",
      " critical p values [1%, 5%, 10%] [-4.50158458 -3.88654012 -3.57500991]\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import ccf, grangercausalitytests, coint, adfuller\n",
    "\n",
    "t_statistic, p_val, critical_p_val = coint(ts1, ts2, trend='ct', autolag=None, maxlag=3)\n",
    "print(f' t statistic: {np.round(t_statistic, 2)} \\n p value: {np.round(p_val,2)} \\n critical p values [1%, 5%, 10%] {critical_p_val}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johansen Test Statistics:\n",
      "[56.70130768 11.12405127]\n",
      "Critical Values (90%, 95%, 99%):\n",
      "[[13.4294 15.4943 19.9349]\n",
      " [ 2.7055  3.8415  6.6349]]\n",
      "Trace statistic for r<=0: 56.70, critical values [13.4294 15.4943 19.9349]\n",
      "Null hypothesis of r<=0 is rejected at the 95% confidence level.\n",
      "Trace statistic for r<=1: 11.12, critical values [2.7055 3.8415 6.6349]\n",
      "Null hypothesis of r<=1 is rejected at the 95% confidence level.\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "# 4. Johansen Co-integration Test\n",
    "data = pd.concat([ts1, ts2], axis=1)\n",
    "result = coint_johansen(data, det_order=0, k_ar_diff=3)\n",
    "\n",
    "print('Johansen Test Statistics:')\n",
    "print(result.lr1)  # Trace statistic\n",
    "print('Critical Values (90%, 95%, 99%):')\n",
    "print(result.cvt)\n",
    "\n",
    "# Interpret the Johansen test results\n",
    "for i in range(len(result.lr1)):\n",
    "    trace_stat = result.lr1[i]\n",
    "    critical_values = result.cvt[i]\n",
    "    print(f'Trace statistic for r<={i}: {trace_stat:.2f}, critical values {critical_values}')\n",
    "\n",
    "    if trace_stat > critical_values[1]:  # Compare to 95% critical value\n",
    "        print(f'Null hypothesis of r<={i} is rejected at the 95% confidence level.')\n",
    "    else:\n",
    "        print(f'Null hypothesis of r<={i} cannot be rejected at the 95% confidence level.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Statistic: -7.971757479514952\n",
      "p-value: 2.769232566258758e-12\n",
      "Critical Values: {'1%': -3.5159766913976376, '5%': -2.898885703483903, '10%': -2.5866935058484217}\n"
     ]
    }
   ],
   "source": [
    "# Extract co-integrating vectors and compute the co-integrating relationship\n",
    "coint_vectors = result.evec\n",
    "coint_vector = coint_vectors[:, 0]  # First co-integrating vector\n",
    "\n",
    "# data = pd.concat([ts1, ts2], axis=1)\n",
    "\n",
    "# Calculate the co-integrating relationship\n",
    "coint_relationship = np.dot(data_s, coint_vector)\n",
    "\n",
    "# Plot the time series and co-integrating relationship on the same plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(ts1, label='Time Series 1', color='blue')\n",
    "plt.plot(ts2, label='Time Series 2', color='red')\n",
    "plt.plot(data.index, coint_relationship, label='Co-integrating Relationship', color='green', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Time Series and Co-integrating Relationship')\n",
    "plt.show()\n",
    "\n",
    "# Test for stationarity of the residuals\n",
    "residuals = coint_relationship - np.mean(coint_relationship)\n",
    "adf_result = adfuller(residuals)\n",
    "print(f'ADF Statistic: {adf_result[0]}')\n",
    "print(f'p-value: {adf_result[1]}')\n",
    "print(f'Critical Values: {adf_result[4]}')\n",
    "\n",
    "# Plot the residuals of the co-integrating relationship on a separate plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(data.index, residuals, label='Residuals of Co-integrating Relationship', color='purple')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('Residuals of Co-integrating Relationship')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: To supress printing the parameers in beast(),      set print.options = 0 \n",
      "INFO: To supress printing the parameers in beast_irreg(),set print.options = 0 \n",
      "INFO: To supress printing the parameers in beast123(),   set extra.printOptions = 0  \n",
      "INFO: To supress warning messages in beast(),            set quiet = 1 \n",
      "INFO: To supress warning messages in beast_irreg(),      set quiet = 1 \n",
      "INFO: To supress warning messages in beast123(),         set extra.quiet = 1  \n",
      "\n",
      "#--------------------------------------------------#\n",
      "#       Brief summary of Input Data                #\n",
      "#--------------------------------------------------#\n",
      "Data Dimension: One signal of length 92\n",
      "IsOrdered     : Yes, ordered in time\n",
      "IsRegular     : Yes, evenly spaced at interval of  0.0833333 year = 1 months = 30.4167 days\n",
      "HasSeasonCmpnt: True  | period = 1 year = 12 months = 365 days. The model 'Y=Trend+Season+Error' is fitted.\n",
      "              : Num_of_DataPoints_per_Period = period/deltaTime = 1/0.0833333 = 12\n",
      "HasOutlierCmpt: False | If true, Y=Trend+Season+Outlier+Error fitted instead of Y=Trend+Season+Error\n",
      "Deseasonalize : False | If true, remove a global seasonal  cmpnt before running BEAST & add it back after BEAST\n",
      "Detrend       : False | If true, remove a global trend component before running BEAST & add it back after BEAST\n",
      "MissingValue  : NaN  flagged as missing values \n",
      "MaxMissingRate: if more than 75% of data is missing, BEAST will skip it.\n",
      "\n",
      "\n",
      "#--------------------------------------------------#\n",
      "#      OPTIONS used in the MCMC inference          #\n",
      "#--------------------------------------------------#\n",
      "\n",
      "#......Start of displaying 'MetaData' ......\n",
      "metadata                =  rb.args() ### or 'lambda: None': just get an empty object### # metadata is used to interpret the input data Y\n",
      "metadata.season         = 'harmonic' # fit a harmonic model to the periodic component\n",
      "metadata.startTime      = 1          # 0001-01-01\n",
      "metadata.deltaTime      = 0.0833333  # 0.0833333 year(s) = 1 month(s) = 30.4167 day(s)\n",
      "metadata.period         = 1          # 1 year(s) = 12 month(s) = 365 day(s) \n",
      "metadata.maxMissingRate = 0.75       # if more than 75% of data is missing, BEAST will skip it.\n",
      "metadata.deseasonalize  = False      # if true,remove a global seasonal cmpnt before running BEAST & add it back later\n",
      "metadata.detrend        = False      # if true,remove a global trend  cmpnt before running BEAST & add it back later\n",
      "#........End of displaying MetaData ........\n",
      "\n",
      "#......Start of displaying 'prior' ......\n",
      "prior                   =  rb.args() ### or 'lambda: None': just get an empty object### # prior is the true model parameters of BEAST\n",
      "prior.seasonMinOrder    = 1          # sorder.minmax[1]: min harmonic order alllowed\n",
      "prior.seasonMaxOrder    = 5          # sorder.minmax[2]: max harmonic order alllowed\n",
      "prior.seasonMinKnotNum  = 0          # scp.minmax[1]   : min num of seasonal chngpts allowed\n",
      "prior.seasonMaxKnotNum  = 10         # scp.minmax[2]   : max num of seasonal chngpts allowed\n",
      "prior.seasonMinSepDist  = 6          # sseg.min        : min seasonal segment length in terms of datapoints\n",
      "prior.seasonLeftMargin  = 6          # sseg.leftmargin : no season chngpts in the first 6 datapoints\n",
      "prior.seasonRightMargin = 6          # sseg.rightmargin: no seoson chngpts in the last 6 datapoints\n",
      "prior.trendMinOrder     = 0          # torder.minmax[1]: min trend polynomial order alllowed\n",
      "prior.trendMaxOrder     = 1          # torder.minmax[2]: max trend polynomial order alllowed\n",
      "prior.trendMinKnotNum   = 0          # tcp.minmax[1]   : min num of chngpts in trend allowed\n",
      "prior.trendMaxKnotNum   = 10         # tcp.minmax[2]   : max num of chngpts in trend allowed\n",
      "prior.trendMinSepDist   = 6          # tseg.min        : min trend segment length in terms of datapoints\n",
      "prior.trendLeftMargin   = 6          # tseg.leftmargin : no trend chngpts in the first 6 datapoints\n",
      "prior.trendRightMargin  = 6          # tseg.rightmargin: no trend chngpts in the last 6 datapoints\n",
      "prior.K_MAX             = 92         # max number of terms in general linear model (relevant only at small values)\n",
      "prior.precValue         = 1.5        # useful mainly when precPriorType='constant'\n",
      "prior.modelPriorType    = 1         \n",
      "prior.precPriorType     = 'componentwise'\n",
      "#......End of displaying prior ......\n",
      "\n",
      "#......Start of displaying 'mcmc' ......\n",
      "mcmc                           =  rb.args() ### or 'lambda: None': just get an empty object### # mcmc is not BEAST parameters but MCMC sampler options\n",
      "mcmc.seed                      = 0          # A nonzero seed to replicate among runs\n",
      "mcmc.samples                   = 8000       # Number of samples saved per chain: the larger, the better\n",
      "mcmc.thinningFactor            = 5          # Thinning the chain: the larger, the better \n",
      "mcmc.burnin                    = 200        # Number of initial samples discarded: the larger, the better\n",
      "mcmc.chainNumber               = 3          # Number of chains: the larger, the better\n",
      "mcmc.maxMoveStepSize           = 6          # Max step of jumping from current changepoint: No need to change\n",
      "mcmc.trendResamplingOrderProb  = 0.1        # Proposal probability of sampling trend polynominal order \n",
      "mcmc.seasonResamplingOrderProb = 0.17       # Proposal probability of sampling seasoanl order \n",
      "mcmc.credIntervalAlphaLevel    = 0.95       # The alphal level for Credible Intervals\n",
      "# Total number of models randomly visited in BEAST is (burnin+sampples*thinFactor)*chainNumber=120600\n",
      "#......End of displaying mcmc ......\n",
      "\n",
      "#......Start of displaying 'extra' ......\n",
      "extra                      =  rb.args() ### or 'lambda: None': just get an empty object### # extra is used to configure output/computing options\n",
      "extra.dumpInputData        = True  # if true, dump a copy of the input data as o.data \n",
      "extra.whichOutputDimIsTime = 1     # 1,2 or 3; which dim of the result is time; used for a 2D/3D input Y\n",
      "extra.computeCredible      = True  # if true, compute  credibiel interval of estimated Y (e.g., o.trend.CI)\n",
      "extra.fastCIComputation    = True  # if true, do not sort but approximiate CI \n",
      "extra.computeSeasonOrder   = True  # if true, dump the estimated time-varying seasonal order: o.season.order \n",
      "extra.computeTrendOrder    = True  # if true, dump the estimated trend polynomial order \n",
      "extra.computeSeasonChngpt  = True  # if true, dump the seasoanl changepoints (scp) in the output \n",
      "extra.computeTrendChngpt   = True  # if true, dump the trend changepoints (tcp) in the output \n",
      "extra.computeSeasonAmp     = False #  compute time-varying seasonal mangitude if season=harmonic  \n",
      "extra.computeTrendSlope    = True  # if true, dump the time-varying slope in trend\n",
      "extra.tallyPosNegSeasonJump= False # differentiate postive/negative jumps at scp\n",
      "extra.tallyPosNegTrendJump = False # differentiate postive/negative jumps at tcp\n",
      "extra.tallyIncDecTrendJump = False # differentiate increased/decreased slopes at tcp\n",
      "extra.printProgressBar     = True  # if true, show an ascii progressbar\n",
      "extra.printOptions         = True  # if true, print the option of the BEAST run\n",
      "extra.consoleWidth         = 85    # an integer specifying the console width for printing\n",
      "extra.numThreadsPerCPU     = 2     # each cpu core spawns 2 concurrent threads (for beast123())\n",
      "extra.numParThreads        = 0     # total number of threads (for beast123() only)\n",
      "#......End of displaying extra ......\n",
      "\n",
      "\\Progress:100.0% done[==============================================================]\n",
      "\n",
      "INFO: To supress printing the parameers in beast(),      set print.options = 0 \n",
      "INFO: To supress printing the parameers in beast_irreg(),set print.options = 0 \n",
      "INFO: To supress printing the parameers in beast123(),   set extra.printOptions = 0  \n",
      "INFO: To supress warning messages in beast(),            set quiet = 1 \n",
      "INFO: To supress warning messages in beast_irreg(),      set quiet = 1 \n",
      "INFO: To supress warning messages in beast123(),         set extra.quiet = 1  \n",
      "\n",
      "#--------------------------------------------------#\n",
      "#       Brief summary of Input Data                #\n",
      "#--------------------------------------------------#\n",
      "Data Dimension: One signal of length 92\n",
      "IsOrdered     : Yes, ordered in time\n",
      "IsRegular     : Yes, evenly spaced at interval of  0.0833333 year = 1 months = 30.4167 days\n",
      "HasSeasonCmpnt: True  | period = 1 year = 12 months = 365 days. The model 'Y=Trend+Season+Error' is fitted.\n",
      "              : Num_of_DataPoints_per_Period = period/deltaTime = 1/0.0833333 = 12\n",
      "HasOutlierCmpt: False | If true, Y=Trend+Season+Outlier+Error fitted instead of Y=Trend+Season+Error\n",
      "Deseasonalize : False | If true, remove a global seasonal  cmpnt before running BEAST & add it back after BEAST\n",
      "Detrend       : False | If true, remove a global trend component before running BEAST & add it back after BEAST\n",
      "MissingValue  : NaN  flagged as missing values \n",
      "MaxMissingRate: if more than 75% of data is missing, BEAST will skip it.\n",
      "\n",
      "\n",
      "#--------------------------------------------------#\n",
      "#      OPTIONS used in the MCMC inference          #\n",
      "#--------------------------------------------------#\n",
      "\n",
      "#......Start of displaying 'MetaData' ......\n",
      "metadata                =  rb.args() ### or 'lambda: None': just get an empty object### # metadata is used to interpret the input data Y\n",
      "metadata.season         = 'harmonic' # fit a harmonic model to the periodic component\n",
      "metadata.startTime      = 1          # 0001-01-01\n",
      "metadata.deltaTime      = 0.0833333  # 0.0833333 year(s) = 1 month(s) = 30.4167 day(s)\n",
      "metadata.period         = 1          # 1 year(s) = 12 month(s) = 365 day(s) \n",
      "metadata.maxMissingRate = 0.75       # if more than 75% of data is missing, BEAST will skip it.\n",
      "metadata.deseasonalize  = False      # if true,remove a global seasonal cmpnt before running BEAST & add it back later\n",
      "metadata.detrend        = False      # if true,remove a global trend  cmpnt before running BEAST & add it back later\n",
      "#........End of displaying MetaData ........\n",
      "\n",
      "#......Start of displaying 'prior' ......\n",
      "prior                   =  rb.args() ### or 'lambda: None': just get an empty object### # prior is the true model parameters of BEAST\n",
      "prior.seasonMinOrder    = 1          # sorder.minmax[1]: min harmonic order alllowed\n",
      "prior.seasonMaxOrder    = 5          # sorder.minmax[2]: max harmonic order alllowed\n",
      "prior.seasonMinKnotNum  = 0          # scp.minmax[1]   : min num of seasonal chngpts allowed\n",
      "prior.seasonMaxKnotNum  = 10         # scp.minmax[2]   : max num of seasonal chngpts allowed\n",
      "prior.seasonMinSepDist  = 6          # sseg.min        : min seasonal segment length in terms of datapoints\n",
      "prior.seasonLeftMargin  = 6          # sseg.leftmargin : no season chngpts in the first 6 datapoints\n",
      "prior.seasonRightMargin = 6          # sseg.rightmargin: no seoson chngpts in the last 6 datapoints\n",
      "prior.trendMinOrder     = 0          # torder.minmax[1]: min trend polynomial order alllowed\n",
      "prior.trendMaxOrder     = 1          # torder.minmax[2]: max trend polynomial order alllowed\n",
      "prior.trendMinKnotNum   = 0          # tcp.minmax[1]   : min num of chngpts in trend allowed\n",
      "prior.trendMaxKnotNum   = 10         # tcp.minmax[2]   : max num of chngpts in trend allowed\n",
      "prior.trendMinSepDist   = 6          # tseg.min        : min trend segment length in terms of datapoints\n",
      "prior.trendLeftMargin   = 6          # tseg.leftmargin : no trend chngpts in the first 6 datapoints\n",
      "prior.trendRightMargin  = 6          # tseg.rightmargin: no trend chngpts in the last 6 datapoints\n",
      "prior.K_MAX             = 92         # max number of terms in general linear model (relevant only at small values)\n",
      "prior.precValue         = 1.5        # useful mainly when precPriorType='constant'\n",
      "prior.modelPriorType    = 1         \n",
      "prior.precPriorType     = 'componentwise'\n",
      "#......End of displaying prior ......\n",
      "\n",
      "#......Start of displaying 'mcmc' ......\n",
      "mcmc                           =  rb.args() ### or 'lambda: None': just get an empty object### # mcmc is not BEAST parameters but MCMC sampler options\n",
      "mcmc.seed                      = 0          # A nonzero seed to replicate among runs\n",
      "mcmc.samples                   = 8000       # Number of samples saved per chain: the larger, the better\n",
      "mcmc.thinningFactor            = 5          # Thinning the chain: the larger, the better \n",
      "mcmc.burnin                    = 200        # Number of initial samples discarded: the larger, the better\n",
      "mcmc.chainNumber               = 3          # Number of chains: the larger, the better\n",
      "mcmc.maxMoveStepSize           = 6          # Max step of jumping from current changepoint: No need to change\n",
      "mcmc.trendResamplingOrderProb  = 0.1        # Proposal probability of sampling trend polynominal order \n",
      "mcmc.seasonResamplingOrderProb = 0.17       # Proposal probability of sampling seasoanl order \n",
      "mcmc.credIntervalAlphaLevel    = 0.95       # The alphal level for Credible Intervals\n",
      "# Total number of models randomly visited in BEAST is (burnin+sampples*thinFactor)*chainNumber=120600\n",
      "#......End of displaying mcmc ......\n",
      "\n",
      "#......Start of displaying 'extra' ......\n",
      "extra                      =  rb.args() ### or 'lambda: None': just get an empty object### # extra is used to configure output/computing options\n",
      "extra.dumpInputData        = True  # if true, dump a copy of the input data as o.data \n",
      "extra.whichOutputDimIsTime = 1     # 1,2 or 3; which dim of the result is time; used for a 2D/3D input Y\n",
      "extra.computeCredible      = True  # if true, compute  credibiel interval of estimated Y (e.g., o.trend.CI)\n",
      "extra.fastCIComputation    = True  # if true, do not sort but approximiate CI \n",
      "extra.computeSeasonOrder   = True  # if true, dump the estimated time-varying seasonal order: o.season.order \n",
      "extra.computeTrendOrder    = True  # if true, dump the estimated trend polynomial order \n",
      "extra.computeSeasonChngpt  = True  # if true, dump the seasoanl changepoints (scp) in the output \n",
      "extra.computeTrendChngpt   = True  # if true, dump the trend changepoints (tcp) in the output \n",
      "extra.computeSeasonAmp     = False #  compute time-varying seasonal mangitude if season=harmonic  \n",
      "extra.computeTrendSlope    = True  # if true, dump the time-varying slope in trend\n",
      "extra.tallyPosNegSeasonJump= False # differentiate postive/negative jumps at scp\n",
      "extra.tallyPosNegTrendJump = False # differentiate postive/negative jumps at tcp\n",
      "extra.tallyIncDecTrendJump = False # differentiate increased/decreased slopes at tcp\n",
      "extra.printProgressBar     = True  # if true, show an ascii progressbar\n",
      "extra.printOptions         = True  # if true, print the option of the BEAST run\n",
      "extra.consoleWidth         = 85    # an integer specifying the console width for printing\n",
      "extra.numThreadsPerCPU     = 2     # each cpu core spawns 2 concurrent threads (for beast123())\n",
      "extra.numParThreads        = 0     # total number of threads (for beast123() only)\n",
      "#......End of displaying extra ......\n",
      "\n",
      "|Progress:100.0% done[==============================================================]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "# import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import Rbeast as rb\n",
    "\n",
    "# Plot original time series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(data['coastline_position_transect_2_waterline'], label='Time Series 1')\n",
    "plt.plot(data['wave_energy_of_combined_wind_waves_and_swell'], label='Time Series 2')\n",
    "plt.legend()\n",
    "plt.title('Original Time Series with Seasonality')\n",
    "plt.show()\n",
    "\n",
    "# Seasonal Decomposition\n",
    "# decomposition_ts1 = seasonal_decompose(data['coastline_position_transect_2_waterline'], model='additive', period=12)\n",
    "# decomposition_ts2 = seasonal_decompose(data['wave_energy_of_combined_wind_waves_and_swell'], model='additive', period=12)\n",
    "decomposition_ts1 = rb.beast(data['coastline_position_transect_2_waterline'], period='1 year', deltat='1/12 year')\n",
    "decomposition_ts2 = rb.beast(data['wave_energy_of_combined_wind_waves_and_swell'], period='1 year', deltat='1/12 year')\n",
    "rb.plot(decomposition_ts2)\n",
    "\n",
    "plot_acf(data['coastline_position_transect_2_waterline'])\n",
    "\n",
    "# # Plot seasonal decomposition\n",
    "# plt.figure(figsize=(14, 12))\n",
    "\n",
    "# plt.subplot(2, 3, 1)\n",
    "# plt.plot(data['coastline_position_transect_2_waterline'], label='Time Series 1')\n",
    "# plt.legend()\n",
    "# plt.title('Time Series 1')\n",
    "\n",
    "# plt.subplot(2, 3, 2)\n",
    "# plt.plot(decomposition_ts1.trend.dropna(), label='Trend Component')\n",
    "# plt.legend()\n",
    "# plt.title('Trend Component of TS1')\n",
    "\n",
    "# plt.subplot(2, 3, 3)\n",
    "# plt.plot(decomposition_ts1.seasonal, label='Seasonal Component')\n",
    "# plt.legend()\n",
    "# plt.title('Seasonal Component of TS1')\n",
    "\n",
    "# plt.subplot(2, 3, 4)\n",
    "# plt.plot(data['wave_energy_of_combined_wind_waves_and_swell'], label='Time Series 2')\n",
    "# plt.legend()\n",
    "# plt.title('Time Series 2')\n",
    "\n",
    "# plt.subplot(2, 3, 5)\n",
    "# plt.plot(decomposition_ts2.trend.dropna(), label='Trend Component')\n",
    "# plt.legend()\n",
    "# plt.title('Trend Component of TS2')\n",
    "\n",
    "# plt.subplot(2, 3, 6)\n",
    "# plt.plot(decomposition_ts2.seasonal, label='Seasonal Component')\n",
    "# plt.legend()\n",
    "# plt.title('Seasonal Component of TS2')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Remove seasonality by subtracting seasonal component\n",
    "# data_deseasonalized = pd.DataFrame({\n",
    "#     'ts1_deseasonalized': data['coastline_position_transect_2_waterline'] - decomposition_ts1.seasonal,\n",
    "#     'ts2_deseasonalized': data['wave_energy_of_combined_wind_waves_and_swell'] - decomposition_ts2.seasonal\n",
    "# })\n",
    "\n",
    "# # Perform Johansen test on deseasonalized data\n",
    "# result_deseasonalized = coint_johansen(data_deseasonalized, det_order=0, k_ar_diff=3)\n",
    "\n",
    "# # Extract co-integrating vectors and compute the co-integrating relationship\n",
    "# coint_vectors_deseasonalized = result_deseasonalized.evec\n",
    "# coint_vector_deseasonalized = coint_vectors_deseasonalized[:, 0]  # First co-integrating vector\n",
    "\n",
    "# # Calculate the co-integrating relationship\n",
    "# coint_relationship_deseasonalized = np.dot(data_deseasonalized, coint_vector_deseasonalized)\n",
    "\n",
    "# # Plot deseasonalized time series and co-integrating relationship\n",
    "# plt.figure(figsize=(14, 6))\n",
    "# # plt.plot(data_deseasonalized['ts1_deseasonalized'], label='Deseasonalized Time Series 1', color='blue')\n",
    "# # plt.plot(data_deseasonalized['ts2_deseasonalized'], label='Deseasonalized Time Series 2', color='red')\n",
    "# plt.plot(data.index, coint_relationship_deseasonalized, label='Co-integrating Relationship', color='green', linestyle='--')\n",
    "# plt.legend()\n",
    "# plt.title('Deseasonalized Time Series and Co-integrating Relationship')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = pd.DataFrame({'ts1_s': decomposition_ts1.season.Y, 'ts2_s': decomposition_ts2.season.Y}, index=ts1.index)\n",
    "data_res = pd.DataFrame({'ts1_res': ts1 - decomposition_ts1.trend.Y - decomposition_ts1.season.Y, 'ts2_res': ts2 - decomposition_ts2.trend.Y - decomposition_ts2.season.Y}, index=ts1.index)\n",
    "data_t = pd.DataFrame({'ts1_t': decomposition_ts1.trend.Y, 'ts2_t': decomposition_ts2.trend.Y}, index=ts1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21a6a08b950>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ts1.index, decomposition_ts2.trend.Y)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(ts1.index, decomposition_ts1.trend.Y, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 640x480 with 9 Axes>,\n",
       " array([<Axes: xlabel='[]', ylabel='Y'>,\n",
       "        <Axes: xlabel='[]', ylabel='season'>,\n",
       "        <Axes: xlabel='[]', ylabel='Pr(scp)'>,\n",
       "        <Axes: xlabel='[]', ylabel='sOrder'>,\n",
       "        <Axes: xlabel='[]', ylabel='trend'>,\n",
       "        <Axes: xlabel='[]', ylabel='Pr(tcp)'>,\n",
       "        <Axes: xlabel='[]', ylabel='tOrder'>,\n",
       "        <Axes: xlabel='[]', ylabel='slpsgn'>,\n",
       "        <Axes: xlabel='time', ylabel='error'>], dtype=object))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.plot(decomposition_ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johansen Test Statistics:\n",
      "[10.22817199  2.17772393]\n",
      "Critical Values (90%, 95%, 99%):\n",
      "[[13.4294 15.4943 19.9349]\n",
      " [ 2.7055  3.8415  6.6349]]\n",
      "Trace statistic for r<=0: 10.23, critical values [13.4294 15.4943 19.9349]\n",
      "Null hypothesis of r<=0 cannot be rejected at the 95% confidence level.\n",
      "Trace statistic for r<=1: 2.18, critical values [2.7055 3.8415 6.6349]\n",
      "Null hypothesis of r<=1 cannot be rejected at the 95% confidence level.\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "# 4. Johansen Co-integration Test\n",
    "result = coint_johansen(data_t, det_order=0, k_ar_diff=1)\n",
    "\n",
    "print('Johansen Test Statistics:')\n",
    "print(result.lr1)  # Trace statistic\n",
    "print('Critical Values (90%, 95%, 99%):')\n",
    "print(result.cvt)\n",
    "\n",
    "\n",
    "# Interpret the Johansen test results\n",
    "for i in range(len(result.lr1)):\n",
    "    trace_stat = result.lr1[i]\n",
    "    critical_values = result.cvt[i]\n",
    "    print(f'Trace statistic for r<={i}: {trace_stat:.2f}, critical values {critical_values}')\n",
    "\n",
    "    if trace_stat > critical_values[1]:  # Compare to 95% critical value\n",
    "        print(f'Null hypothesis of r<={i} is rejected at the 95% confidence level.')\n",
    "    else:\n",
    "        print(f'Null hypothesis of r<={i} cannot be rejected at the 95% confidence level.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IslandTimeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
